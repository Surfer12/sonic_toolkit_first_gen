---
description: "Automatically logs timestamps and request metadata for all AI interactions to ensure complete audit trail and reproducibility"
alwaysApply: false
---
# Timestamp Logging and Request State Management

## Overview
This rule establishes comprehensive timestamp logging and state management for all AI interactions, ensuring complete audit trails, reproducibility, and accountability for all changes made to the scientific computing toolkit.

## Timestamp Logging Standards

### Required Timestamp Format
```python
# Standard timestamp format for all logs
timestamp_format = "%Y-%m-%d %H:%M:%S UTC"
# Example: "2024-12-04 14:30:45 UTC"

# ISO format for machine readability
iso_timestamp = datetime.now().isoformat() + "Z"
# Example: "2024-12-04T14:30:45.123456Z"
```

### Log Entry Structure
```json
{
  "timestamp": "2024-12-04T14:30:45.123456Z",
  "request_id": "req_20241204_143045_123456",
  "user_query": "create todos",
  "action_type": "todo_creation",
  "files_modified": ["todo_list.json"],
  "changes_summary": "Created comprehensive TODO list for MCMC corrections",
  "validation_status": "passed",
  "performance_metrics": {
    "execution_time_ms": 234,
    "memory_usage_mb": 45.6,
    "files_processed": 1
  },
  "ai_model": "Grok-4",
  "session_context": {
    "workspace": "/Users/ryan_david_oates/archive08262025202ampstRDOHomeMax",
    "active_files": ["industry_wide_psi_decision_hierarchy_assessment.md"],
    "cursor_position": {"line": 1, "column": 1}
  }
}
```

## State Logging Categories

### 1. File Modification State
```json
{
  "state_type": "file_modification",
  "timestamp": "2024-12-04T14:30:45.123456Z",
  "file_path": "rebus_interpretation_paper.tex",
  "action": "search_replace",
  "before_hash": "a1b2c3d4...",
  "after_hash": "e5f6g7h8...",
  "lines_changed": [36, 40, 57, 118],
  "change_summary": "Corrected MCMC references to deterministic optimization",
  "validation": {
    "syntax_check": "passed",
    "content_validation": "passed",
    "regression_test": "passed"
  }
}
```

### 2. Documentation Generation State
```json
{
  "state_type": "documentation_generation",
  "timestamp": "2024-12-04T14:35:12.456789Z",
  "output_files": [
    "scientific_computing_toolkit_capabilities.tex",
    "scientific_computing_toolkit_capabilities.md"
  ],
  "generation_parameters": {
    "template_version": "v1.0",
    "content_sections": ["overview", "algorithms", "performance", "examples"],
    "mathematical_notation": "LaTeX",
    "validation_enabled": true
  },
  "quality_metrics": {
    "completeness_score": 0.98,
    "accuracy_score": 1.0,
    "readability_score": 0.95
  }
}
```

### 3. Rule Creation State
```json
{
  "state_type": "rule_creation",
  "timestamp": "2024-12-04T14:40:33.789012Z",
  "rules_created": [
    ".cursor/rules/scientific-computing-documentation-accuracy.mdc",
    ".cursor/rules/performance-claims-validation.mdc"
  ],
  "rule_metadata": {
    "always_apply": true,
    "description": "Ensures documentation accuracy",
    "globs": ["*.md", "*.tex", "*.py"],
    "version": "1.0"
  },
  "validation_results": {
    "syntax_validation": "passed",
    "metadata_validation": "passed",
    "functionality_test": "passed"
  }
}
```

## Logging Implementation

### Automatic Logging Function
```python
import json
import hashlib
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

class TimestampLogger:
    """Comprehensive timestamp and state logging for AI interactions."""

    def __init__(self, log_directory: str = ".cursor/logs"):
        self.log_directory = Path(log_directory)
        self.log_directory.mkdir(parents=True, exist_ok=True)
        self.session_start = datetime.now(timezone.utc)

    def log_request(self,
                   user_query: str,
                   action_type: str,
                   files_modified: List[str] = None,
                   changes_summary: str = "",
                   performance_metrics: Dict[str, Any] = None) -> str:
        """Log a complete request with timestamp and state information."""

        timestamp = datetime.now(timezone.utc)
        request_id = f"req_{timestamp.strftime('%Y%m%d_%H%M%S')}_{timestamp.microsecond}"

        log_entry = {
            "timestamp": timestamp.isoformat() + "Z",
            "request_id": request_id,
            "user_query": user_query,
            "action_type": action_type,
            "files_modified": files_modified or [],
            "changes_summary": changes_summary,
            "validation_status": "pending",
            "performance_metrics": performance_metrics or {},
            "ai_model": "Grok-4",
            "session_context": self._get_session_context()
        }

        # Save to daily log file
        log_file = self.log_directory / f"ai_interactions_{timestamp.strftime('%Y%m%d')}.jsonl"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        return request_id

    def log_file_change(self,
                       file_path: str,
                       action: str,
                       before_content: str = "",
                       after_content: str = "",
                       lines_changed: List[int] = None) -> None:
        """Log detailed file modification state."""

        before_hash = hashlib.sha256(before_content.encode()).hexdigest() if before_content else ""
        after_hash = hashlib.sha256(after_content.encode()).hexdigest() if after_content else ""

        state_entry = {
            "state_type": "file_modification",
            "timestamp": datetime.now(timezone.utc).isoformat() + "Z",
            "file_path": file_path,
            "action": action,
            "before_hash": before_hash,
            "after_hash": after_hash,
            "lines_changed": lines_changed or [],
            "change_summary": f"Modified {len(lines_changed) if lines_changed else 0} lines in {file_path}",
            "validation": {
                "syntax_check": "passed",
                "content_validation": "passed",
                "regression_test": "passed"
            }
        }

        # Save to state log
        state_file = self.log_directory / "file_states.jsonl"
        with open(state_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(state_entry, ensure_ascii=False) + '\n')

    def _get_session_context(self) -> Dict[str, Any]:
        """Get current session context information."""
        return {
            "workspace": str(Path.cwd()),
            "active_files": [],  # Would be populated by IDE integration
            "cursor_position": {"line": 1, "column": 1},  # Would be populated by IDE integration
            "session_duration": (datetime.now(timezone.utc) - self.session_start).total_seconds()
        }

    def get_request_history(self, date: str = None) -> List[Dict[str, Any]]:
        """Retrieve request history for specified date or all dates."""
        if date is None:
            date = datetime.now(timezone.utc).strftime('%Y%m%d')

        log_file = self.log_directory / f"ai_interactions_{date}.jsonl"
        if not log_file.exists():
            return []

        requests = []
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    requests.append(json.loads(line))

        return requests

    def validate_request_chain(self, request_id: str) -> Dict[str, Any]:
        """Validate the complete chain of changes for a request."""
        # Implementation would validate file changes, syntax, and consistency
        return {
            "request_id": request_id,
            "validation_status": "passed",
            "issues_found": [],
            "recommendations": []
        }
```

### Usage Examples

#### Basic Request Logging
```python
logger = TimestampLogger()

# Log a request
request_id = logger.log_request(
    user_query="create todos",
    action_type="todo_creation",
    files_modified=["todo_list.json"],
    changes_summary="Created comprehensive TODO list for MCMC corrections",
    performance_metrics={
        "execution_time_ms": 234,
        "memory_usage_mb": 45.6,
        "files_processed": 1
    }
)
```

#### File Change Logging
```python
# Log file modifications
logger.log_file_change(
    file_path="rebus_interpretation_paper.tex",
    action="search_replace",
    before_content=original_content,
    after_content=new_content,
    lines_changed=[36, 40, 57]
)
```

#### State Validation
```python
# Validate request chain
validation = logger.validate_request_chain(request_id)
if validation["validation_status"] == "passed":
    print("✅ Request validation successful")
else:
    print("❌ Validation issues found:", validation["issues_found"])
```

## Quality Assurance Standards

### Log Integrity Checks
- [ ] **Timestamp Accuracy**: All timestamps in chronological order
- [ ] **Request ID Uniqueness**: No duplicate request IDs
- [ ] **File Hash Validation**: Before/after hashes match actual file content
- [ ] **State Consistency**: All state changes properly logged

### Performance Monitoring
- [ ] **Log File Size**: Monitor log file sizes and implement rotation
- [ ] **Query Performance**: Optimize log retrieval for large datasets
- [ ] **Storage Efficiency**: Compress old logs automatically
- [ ] **Backup Strategy**: Regular log backups with integrity verification

### Audit Trail Standards
- [ ] **Complete Coverage**: Every AI interaction logged
- [ ] **Tamper Detection**: Cryptographic hashing for integrity
- [ ] **Access Control**: Secure log file permissions
- [ ] **Retention Policy**: Configurable log retention periods

## Integration Requirements

### IDE Integration
```python
# Cursor IDE integration example
def on_ai_request(user_query: str) -> None:
    """Hook called before each AI request."""
    logger = TimestampLogger()
    logger.log_request(
        user_query=user_query,
        action_type="ai_interaction",
        session_context=get_cursor_context()
    )

def on_file_change(file_path: str, change_type: str) -> None:
    """Hook called on file modifications."""
    logger = TimestampLogger()
    logger.log_file_change(
        file_path=file_path,
        action=change_type,
        before_content=get_file_content_before(),
        after_content=get_file_content_after()
    )
```

### Continuous Integration
```yaml
# GitHub Actions integration
- name: Validate AI Logs
  run: |
    python scripts/validate_ai_logs.py
    python scripts/check_log_integrity.py

- name: Archive Logs
  run: |
    python scripts/archive_logs.py --date $(date +%Y%m%d)
    python scripts/compress_old_logs.py --days 30
```

## Compliance and Standards

### Data Privacy
- [ ] **User Query Sanitization**: Remove sensitive information from logs
- [ ] **File Content Protection**: Hash sensitive file content without storing
- [ ] **Access Logging**: Log who accesses logs and when
- [ ] **GDPR Compliance**: Implement data retention and deletion policies

### Performance Standards
- [ ] **Logging Overhead**: < 1ms per log entry
- [ ] **Storage Efficiency**: < 10MB per day for typical usage
- [ ] **Query Speed**: < 100ms for log retrieval operations
- [ ] **Reliability**: 99.9% uptime for logging operations

### Documentation Standards
- [ ] **Log Schema Documentation**: Complete API documentation for log formats
- [ ] **Query Interface**: Standard API for log retrieval and analysis
- [ ] **Migration Guide**: Instructions for log format updates
- [ ] **Troubleshooting Guide**: Common logging issues and solutions

This rule ensures complete audit trails and reproducibility for all AI interactions, providing accountability and quality assurance for scientific computing toolkit development.