---
globs: *benchmark*.py,benchmark*.py,*performance*.py,performance*.py,*validation*.py,validation*.py
description: "Performance benchmarking standards and validation frameworks for scientific computing"
---

# ⚡ Performance Benchmarking Standards

## Overview
The Scientific Computing Toolkit implements comprehensive performance benchmarking standards to ensure reproducible, validated, and optimized scientific computing results. This rule applies to all benchmarking, performance analysis, and validation code.

## Core Benchmarking Framework

### Performance Metrics Hierarchy
```
Primary Metrics (Always Required)
├── Execution Time: Wall-clock time for complete operation
├── Memory Usage: Peak and average memory consumption
├── CPU Utilization: Processor usage percentage and patterns
└── Scalability: Performance scaling with problem size

Secondary Metrics (Domain-Specific)
├── Accuracy: Numerical precision and error bounds
├── Convergence: Iteration counts and convergence rates
├── Efficiency: Operations per second and resource utilization
└── Robustness: Performance under varying conditions
```

### Benchmark Execution Standards

#### Setup Requirements
```python
# Standard benchmark setup pattern
import time
import psutil
import tracemalloc
from performance_benchmarking import PerformanceBenchmarker

def benchmark_function():
    """
    Standard benchmarking function template.
    Must include all required metrics and validation.
    """

    # Initialize benchmarker
    benchmarker = PerformanceBenchmarker()

    # Memory tracking setup
    tracemalloc.start()
        process = psutil.Process()

    # Pre-benchmark measurements
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    time_start = time.perf_counter()

    try:
        # === BENCHMARKED CODE SECTION ===
        result = target_function(*args, **kwargs)
        # ================================

        # Post-benchmark measurements
        time_end = time.perf_counter()
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        peak_memory = tracemalloc.get_traced_memory()[1] / 1024 / 1024  # MB

        # Calculate metrics
        execution_time = time_end - time_start
        memory_used = memory_after - memory_before
        cpu_percent = process.cpu_percent(interval=1.0)

        # Validation and reporting
        benchmark_result = benchmarker.create_benchmark_result(
            execution_time=execution_time,
            memory_used=memory_used,
            peak_memory=peak_memory,
            cpu_percent=cpu_percent,
            result=result,
            metadata={
                'function': target_function.__name__,
                'args': str(args),
                'kwargs': str(kwargs),
                'timestamp': time.time()
            }
        )

        return benchmark_result

        finally:
            tracemalloc.stop()
```

#### Validation Requirements
```python
# Benchmark validation standards
def validate_benchmark_results(results, expected_metrics):
    """
    Validate benchmark results against expected performance metrics.

    Args:
        results: BenchmarkResult object
        expected_metrics: Dict of expected performance ranges

    Returns:
        ValidationReport with pass/fail status
    """

    validation_report = {
        'execution_time': {
            'actual': results.execution_time,
            'expected_range': expected_metrics.get('execution_time', (0, float('inf'))),
            'status': 'PASS' if expected_metrics['execution_time'][0] <= results.execution_time <= expected_metrics['execution_time'][1] else 'FAIL'
        },
        'memory_usage': {
            'actual': results.memory_used,
            'expected_max': expected_metrics.get('max_memory', float('inf')),
            'status': 'PASS' if results.memory_used <= expected_metrics.get('max_memory', float('inf')) else 'FAIL'
        },
        'accuracy': {
            'actual': results.accuracy if hasattr(results, 'accuracy') else None,
            'expected_min': expected_metrics.get('min_accuracy', 0),
            'status': 'PASS' if results.accuracy >= expected_metrics.get('min_accuracy', 0) else 'UNKNOWN'
        }
    }

    overall_status = 'PASS' if all(v['status'] == 'PASS' for v in validation_report.values()) else 'FAIL'

        return {
        'validation_report': validation_report,
        'overall_status': overall_status,
        'recommendations': generate_recommendations(validation_report)
    }
```

## Performance Targets and Standards

### Target Performance Metrics
- **Execution Time**: < 0.1s for small problems, < 1.0s for medium problems, < 10.0s for large problems
- **Memory Usage**: < 100 MB for small problems, < 1 GB for medium problems, < 10 GB for large problems
- **CPU Utilization**: 70-90% sustained utilization for compute-intensive operations
- **Scalability**: O(n) to O(n²) complexity acceptable, O(n³+) requires optimization
- **Accuracy**: >99% for validation benchmarks, >95% for application benchmarks

### Quality Assurance Standards
- **Reproducibility**: Benchmarks must produce consistent results (±5% variation)
- **Statistical Significance**: Performance differences >10% considered significant
- **Regression Detection**: Automatic alerts for >5% performance degradation
- **Documentation**: All benchmarks must include comprehensive documentation

## Benchmark Categories

### 1. Unit Benchmarks
**Purpose**: Test individual functions and algorithms
**Scope**: Single operations, small data sizes
**Duration**: < 1 second per benchmark
**Metrics**: Execution time, memory usage, basic accuracy

### 2. Integration Benchmarks
**Purpose**: Test component interactions and workflows
**Scope**: Multi-component operations, medium data sizes
**Duration**: 1-60 seconds per benchmark
**Metrics**: End-to-end performance, resource utilization, throughput

### 3. System Benchmarks
**Purpose**: Test complete system performance
**Scope**: Full workflows, large data sizes
**Duration**: 1-600 seconds per benchmark
**Metrics**: Overall performance, scalability, robustness

### 4. Regression Benchmarks
**Purpose**: Detect performance changes over time
**Scope**: Historical comparison of identical operations
**Duration**: Variable, run periodically
**Metrics**: Performance deltas, trend analysis, anomaly detection

## Statistical Analysis Standards

### Performance Distribution Analysis
```python
def analyze_performance_distribution(times, confidence_level=0.95):
    """
    Analyze performance distribution for statistical significance.

    Args:
        times: List of execution times
        confidence_level: Statistical confidence level

    Returns:
        Statistical analysis results
    """

    import numpy as np
    from scipy import stats

    # Basic statistics
    mean_time = np.mean(times)
    std_time = np.std(times, ddof=1)
    median_time = np.median(times)

    # Confidence interval
    n = len(times)
    t_value = stats.t.ppf((1 + confidence_level) / 2, n - 1)
    margin_error = t_value * std_time / np.sqrt(n)
    ci_lower = mean_time - margin_error
    ci_upper = mean_time + margin_error

    # Normality test
    _, normality_p = stats.shapiro(times)
    is_normal = normality_p > 0.05

    # Outlier detection
    q1, q3 = np.percentile(times, [25, 75])
    iqr = q3 - q1
    outlier_bounds = (q1 - 1.5 * iqr, q3 + 1.5 * iqr)
    outliers = [t for t in times if t < outlier_bounds[0] or t > outlier_bounds[1]]

    return {
        'mean': mean_time,
        'std': std_time,
        'median': median_time,
        'confidence_interval': (ci_lower, ci_upper),
        'normality_test': {'p_value': normality_p, 'is_normal': is_normal},
        'outliers': outliers,
        'sample_size': n,
        'confidence_level': confidence_level
    }
```

### Scalability Analysis
```python
def analyze_scalability(problem_sizes, execution_times):
    """
    Analyze performance scalability with problem size.

    Args:
        problem_sizes: List of problem sizes (n)
        execution_times: Corresponding execution times

    Returns:
        Scalability analysis results
    """

    import numpy as np

    # Log-linear regression for complexity analysis
        log_sizes = np.log(problem_sizes)
        log_times = np.log(execution_times)

    # Fit scaling law: time = a * size^b
    coeffs = np.polyfit(log_sizes, log_times, 1)
    scaling_exponent = coeffs[0]  # b in time = a * size^b

    # Determine complexity class
    if scaling_exponent < 1.1:
        complexity_class = "O(n) - Linear"
    elif scaling_exponent < 1.3:
        complexity_class = "O(n log n) - Linearithmic"
    elif scaling_exponent < 2.1:
        complexity_class = "O(n²) - Quadratic"
    elif scaling_exponent < 3.1:
        complexity_class = "O(n³) - Cubic"
    else:
        complexity_class = f"O(n^{scaling_exponent:.1f}) - Higher Order"

    # Efficiency assessment
    operations_per_second = np.array(problem_sizes) / np.array(execution_times)

    return {
        'scaling_exponent': scaling_exponent,
        'complexity_class': complexity_class,
        'efficiency_trend': operations_per_second,
        'regression_coefficients': coeffs,
        'r_squared': np.corrcoef(log_sizes, log_times)[0, 1]**2
    }
```

## Benchmark Documentation Standards

### Benchmark Report Template
```markdown
# Benchmark Report: [Benchmark Name]

## Executive Summary
[Benchmark purpose and key findings - 2-3 sentences]

## Benchmark Configuration
- **Date**: [YYYY-MM-DD]
- **System**: [Hardware and software specifications]
- **Parameters**: [Key benchmark parameters]
- **Iterations**: [Number of benchmark runs]

## Performance Results

### Primary Metrics
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Execution Time | [value] | [target] | ✅/❌ |
| Memory Usage | [value] | [target] | ✅/❌ |
| CPU Utilization | [value] | [target] | ✅/❌ |

### Statistical Analysis
- **Mean**: [value] ± [std]
- **Median**: [value]
- **95% CI**: [[lower], [upper]]
- **Normality**: [p-value] ([PASS/FAIL])
- **Outliers**: [count] detected

### Scalability Analysis
- **Complexity**: [O(n^x)] - [classification]
- **Efficiency Trend**: [description]
- **Regression R²**: [value]

## Validation Results
- **Accuracy**: [value]% ([PASS/FAIL])
- **Precision**: [value] ([PASS/FAIL])
- **Robustness**: [assessment]

## Recommendations
1. [Primary optimization opportunity]
2. [Secondary improvement area]
3. [Long-term enhancement suggestion]

## Benchmark Artifacts
- Raw data: `benchmark_data_[date].json`
- Plots: `benchmark_plots_[date].png`
- Configuration: `benchmark_config_[date].yaml`
```

## Continuous Integration Standards

### CI/CD Benchmark Integration
```yaml
# .github/workflows/benchmark-validation.yml
name: Benchmark Validation
on:
  pull_request:
    paths:
      - 'src/**/*.py'
      - 'benchmarks/**/*.py'
  schedule:
    - cron: '0 2 * * 1'  # Weekly benchmark validation

jobs:
  benchmark-validation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit benchmarks
        run: python -m pytest benchmarks/unit/ -v --tb=short

      - name: Run integration benchmarks
        run: python -m pytest benchmarks/integration/ -v --tb=short

      - name: Performance regression check
        run: |
          python scripts/check_performance_regression.py --baseline=main

      - name: Generate benchmark report
        run: python scripts/generate_benchmark_report.py

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark_reports/
```

## Maintenance and Evolution

### Benchmark Maintenance Schedule
- **Daily**: Automated regression detection
- **Weekly**: Full benchmark suite execution
- **Monthly**: Performance trend analysis and optimization
- **Quarterly**: Benchmark suite updates and new test cases

### Performance Baseline Updates
- Update baselines after significant code changes
- Document performance improvement justifications
- Maintain historical performance data for trend analysis
- Alert team to performance anomalies

### Documentation Updates
- Update benchmark documentation with code changes
- Maintain benchmark configuration documentation
- Document performance optimization techniques
- Share best practices across team

This comprehensive benchmarking framework ensures consistent, reliable, and well-documented performance evaluation across all scientific computing toolkit components.