---
alwaysApply: false
description: "Standards for Blackwell MXFP8 hardware integration in scientific computing algorithms, ensuring optimal precision and performance"
globs: *.py,*.cu,*.cpp
---

# Blackwell MXFP8 Integration Standards

## Overview
This rule establishes comprehensive standards for integrating Blackwell MXFP8 hardware acceleration into scientific computing algorithms, ensuring optimal precision preservation, performance gains, and compatibility with the toolkit's 0.9987 convergence requirements.

## MXFP8 Format Fundamentals

### Precision Specifications
```python
# ✅ CORRECT: MXFP8 format specifications
MXFP8_E4M3_SPEC = {
    'exponent_bits': 4,
    'mantissa_bits': 3,
    'bias': 8,
    'range': [-448.0, 448.0],
    'precision': '1.6 × 10^-1',  # Relative precision
    'special_values': {
        'zero': '00000000',
        'infinity': '01111111',
        'NaN': '11111111'
    }
}

MXFP8_E5M2_SPEC = {
    'exponent_bits': 5,
    'mantissa_bits': 2,
    'bias': 16,
    'range': [-57344.0, 57344.0],
    'precision': '2.5 × 10^-1',  # Relative precision
    'special_values': {
        'zero': '00000000',
        'infinity': '01111111',
        'NaN': '11111111'
    }
}
```

### Quantization Standards
```python
# ✅ CORRECT: Blackwell MXFP8 quantization implementation
def quantize_blackwell_mxfp8(tensor_fp32, format='E4M3'):
    """
    Quantize FP32 tensor to Blackwell MXFP8 format.

    Parameters:
    -----------
    tensor_fp32 : torch.Tensor
        Input tensor in FP32 precision
    format : str
        MXFP8 format ('E4M3' or 'E5M2')

    Returns:
    --------
    tensor_mxfp8 : torch.Tensor
        Quantized tensor in MXFP8 format
    """
    if format == 'E4M3':
        # E4M3 quantization: 4-bit exponent, 3-bit mantissa
        return torch.quantize_per_tensor(
            tensor_fp32,
            scale=1.0 / 448.0,  # Max absolute value for E4M3
            zero_point=0,
            dtype=torch.float8_e4m3fn
        )
    elif format == 'E5M2':
        # E5M2 quantization: 5-bit exponent, 2-bit mantissa
        return torch.quantize_per_tensor(
            tensor_fp32,
            scale=1.0 / 57344.0,  # Max absolute value for E5M2
            zero_point=0,
            dtype=torch.float8_e5m2
        )
    else:
        raise ValueError(f"Unsupported MXFP8 format: {format}")

def dequantize_blackwell_mxfp8(tensor_mxfp8):
    """
    Dequantize MXFP8 tensor back to FP32.

    Parameters:
    -----------
    tensor_mxfp8 : torch.Tensor
        Input tensor in MXFP8 format

    Returns:
    --------
    tensor_fp32 : torch.Tensor
        Dequantized tensor in FP32 format
    """
    return tensor_mxfp8.dequantize()
```

## Hardware Acceleration Standards

### Blackwell Architecture Utilization
```python
# ✅ CORRECT: Blackwell hardware context management
class BlackwellMXFP8Context:
    """
    Context manager for Blackwell MXFP8 optimization.

    Ensures proper hardware utilization and precision preservation.
    """

    def __init__(self, precision_target=0.999744, memory_limit_gb=192):
        self.precision_target = precision_target
        self.memory_limit_gb = memory_limit_gb
        self.original_precision = None

    def __enter__(self):
        # Store original precision settings
        self.original_precision = torch.get_default_dtype()

        # Configure Blackwell MXFP8
        torch.set_default_dtype(torch.float8_e4m3fn)

        # Enable Blackwell-specific optimizations
        if torch.cuda.is_available() and torch.cuda.get_device_name(0).startswith('H100'):
            torch.backends.cuda.matmul.allow_mxfp8 = True
            torch.backends.cuda.matmul.allow_fp8_reduction = True

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Restore original precision settings
        if self.original_precision is not None:
            torch.set_default_dtype(self.original_precision)

        # Reset Blackwell-specific optimizations
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_mxfp8 = False
            torch.backends.cuda.matmul.allow_fp8_reduction = False

# Usage example
with BlackwellMXFP8Context() as ctx:
    # All operations within this context use MXFP8
    result = blackwell_optimized_computation(data)
```

### Tensor Core Optimization
```python
# ✅ CORRECT: Blackwell tensor core utilization
def blackwell_tensor_core_matmul(A_mxfp8, B_mxfp8):
    """
    Optimized matrix multiplication using Blackwell tensor cores.

    Parameters:
    -----------
    A_mxfp8, B_mxfp8 : torch.Tensor
        Input matrices in MXFP8 format

    Returns:
    --------
    C_fp32 : torch.Tensor
        Result in FP32 format with accumulated precision
    """
    # Ensure Blackwell tensor cores are available
    assert torch.cuda.is_available(), "CUDA required for Blackwell optimization"
    assert torch.cuda.get_device_name(0).startswith('H100'), "Blackwell GPU required"

    # Configure for MXFP8 operations
    with torch.cuda.device(0):
        # Enable Blackwell tensor core features
        torch.backends.cuda.matmul.allow_mxfp8 = True
        torch.backends.cuda.matmul.allow_fp8_reduction = True

        # Perform matrix multiplication with MXFP8 accumulation
        C_mxfp8 = torch.matmul(A_mxfp8, B_mxfp8)

        # Convert result back to FP32 for precision-critical operations
        C_fp32 = C_mxfp8.to(torch.float32)

    return C_fp32
```

## Precision Preservation Standards

### Accuracy Validation Framework
```python
# ✅ CORRECT: Precision validation for MXFP8 operations
def validate_mxfp8_precision(original_fp32, mxfp8_result, tolerance=0.999744):
    """
    Validate that MXFP8 operations maintain required precision.

    Parameters:
    -----------
    original_fp32 : torch.Tensor
        Reference result in FP32
    mxfp8_result : torch.Tensor
        MXFP8 computation result
    tolerance : float
        Required correlation tolerance (default: Blackwell target)

    Returns:
    --------
    validation_result : dict
        Precision validation metrics
    """
    # Compute correlation coefficient
    correlation = torch.corrcoef(
        torch.stack([original_fp32.flatten(), mxfp8_result.flatten()])
    )[0, 1].item()

    # Compute relative error
    relative_error = torch.mean(
        torch.abs(original_fp32 - mxfp8_result) / (torch.abs(original_fp32) + 1e-8)
    ).item()

    # Blackwell MXFP8 precision targets
    precision_maintained = correlation >= tolerance
    error_acceptable = relative_error <= (1.0 - tolerance)

    return {
        'correlation_coefficient': correlation,
        'relative_error': relative_error,
        'precision_target_met': precision_maintained,
        'error_target_met': error_acceptable,
        'blackwell_compliant': precision_maintained and error_acceptable,
        'performance_gain': compute_performance_gain(original_fp32, mxfp8_result)
    }

def compute_performance_gain(reference_time, mxfp8_time):
    """Compute performance improvement from MXFP8 optimization."""
    return reference_time / mxfp8_time if mxfp8_time > 0 else float('inf')
```

### Error Bounds Analysis
```python
# ✅ CORRECT: MXFP8 error bounds calculation
def analyze_mxfp8_error_bounds(tensor_fp32, format='E4M3'):
    """
    Analyze theoretical error bounds for MXFP8 quantization.

    Parameters:
    -----------
    tensor_fp32 : torch.Tensor
        Original FP32 tensor
    format : str
        MXFP8 format specification

    Returns:
    --------
    error_analysis : dict
        Theoretical and empirical error bounds
    """
    if format == 'E4M3':
        # E4M3: 4-bit exponent, 3-bit mantissa
        mantissa_bits = 3
        exponent_bias = 8
        max_value = 2**(2**4 - exponent_bias) * (1 + (2**3 - 1)/2**3)
    elif format == 'E5M2':
        # E5M2: 5-bit exponent, 2-bit mantissa
        mantissa_bits = 2
        exponent_bias = 16
        max_value = 2**(2**5 - exponent_bias) * (1 + (2**2 - 1)/2**2)
    else:
        raise ValueError(f"Unsupported MXFP8 format: {format}")

    # Theoretical relative precision
    theoretical_precision = 2**(-mantissa_bits - 1)

    # Empirical error analysis
    quantized = quantize_blackwell_mxfp8(tensor_fp32, format)
    dequantized = dequantize_blackwell_mxfp8(quantized)

    empirical_error = torch.mean(
        torch.abs(tensor_fp32 - dequantized) / (torch.abs(tensor_fp32) + 1e-8)
    ).item()

    return {
        'theoretical_precision': theoretical_precision,
        'empirical_error': empirical_error,
        'error_within_bounds': empirical_error <= theoretical_precision * 2,
        'blackwell_precision_target': empirical_error <= 0.000256,  # Blackwell requirement
        'quantization_range': [-max_value, max_value]
    }
```

## Memory Management Standards

### Blackwell TMEM Utilization
```python
# ✅ CORRECT: Blackwell tensor memory management
def optimize_blackwell_memory_layout(tensor, target_layout='TMEM_OPTIMAL'):
    """
    Optimize tensor layout for Blackwell TMEM utilization.

    Parameters:
    -----------
    tensor : torch.Tensor
        Input tensor to optimize
    target_layout : str
        Target memory layout ('TMEM_OPTIMAL', 'HBM_OPTIMAL')

    Returns:
    --------
    optimized_tensor : torch.Tensor
        Tensor with optimized memory layout
    """
    if target_layout == 'TMEM_OPTIMAL':
        # Optimize for Blackwell's 128×512 TMEM
        # Ensure tensor dimensions align with TMEM layout
        optimized_shape = optimize_for_tmem(tensor.shape)

        # Reshape if beneficial for TMEM access patterns
        if optimized_shape != tensor.shape:
            tensor = tensor.view(optimized_shape)

        # Ensure memory alignment for TMEM access
        tensor = tensor.contiguous(memory_format=torch.contiguous_format)

    elif target_layout == 'HBM_OPTIMAL':
        # Optimize for 192GB HBM3e bandwidth
        # Maximize memory bandwidth utilization
        tensor = tensor.pin_memory()  # Pin for faster HBM access

    return tensor

def optimize_for_tmem(original_shape):
    """
    Optimize tensor shape for Blackwell TMEM access patterns.

    Blackwell TMEM: 128×512 layout with 4th-gen tensor cores
    """
    # Analyze current shape for TMEM compatibility
    if len(original_shape) == 2:
        m, n = original_shape
        # Optimize for 128×512 TMEM layout
        if m % 128 == 0 and n % 512 == 0:
            return original_shape  # Already optimal

        # Find nearest optimal dimensions
        optimal_m = round(m / 128) * 128
        optimal_n = round(n / 512) * 512

        return (max(optimal_m, 128), max(optimal_n, 512))
    else:
        # For higher-dimensional tensors, optimize leading dimensions
        return original_shape  # Return as-is for complex cases
```

## Performance Benchmarking Standards

### Blackwell MXFP8 Benchmark Template
```python
# ✅ CORRECT: Comprehensive MXFP8 benchmarking
def benchmark_blackwell_mxfp8_performance(problem_sizes, algorithms):
    """
    Comprehensive benchmarking of Blackwell MXFP8 performance.

    Parameters:
    -----------
    problem_sizes : list
        List of problem sizes to benchmark
    algorithms : list
        List of algorithms to benchmark

    Returns:
    --------
    benchmark_results : dict
        Comprehensive performance analysis
    """
    results = {}

    for size in problem_sizes:
        for algorithm in algorithms:
            # Generate test problem
            problem = generate_benchmark_problem(size, algorithm)

            # Benchmark FP32 baseline
            fp32_time, fp32_result = benchmark_fp32(problem, algorithm)

            # Benchmark Blackwell MXFP8
            with BlackwellMXFP8Context() as ctx:
                mxfp8_time, mxfp8_result = benchmark_mxfp8(problem, algorithm)

            # Validate precision preservation
            precision_analysis = validate_mxfp8_precision(
                fp32_result, mxfp8_result
            )

            # Analyze memory usage
            memory_analysis = analyze_memory_usage(problem, algorithm)

            results[f"{algorithm}_size_{size}"] = {
                'fp32_time': fp32_time,
                'mxfp8_time': mxfp8_time,
                'speedup': fp32_time / mxfp8_time,
                'precision_correlation': precision_analysis['correlation_coefficient'],
                'precision_maintained': precision_analysis['blackwell_compliant'],
                'memory_fp32': memory_analysis['fp32_usage'],
                'memory_mxfp8': memory_analysis['mxfp8_usage'],
                'energy_efficiency': memory_analysis['energy_ratio'],
                'blackwell_compliant': (
                    precision_analysis['blackwell_compliant'] and
                    fp32_time / mxfp8_time >= 3.4  # Blackwell speedup target
                )
            }

    return results
```

### Scalability Analysis
```python
# ✅ CORRECT: Blackwell scalability analysis
def analyze_blackwell_scalability(max_problem_size, scaling_factors):
    """
    Analyze Blackwell MXFP8 scalability across problem sizes.

    Parameters:
    -----------
    max_problem_size : int
        Maximum problem size to analyze
    scaling_factors : list
        Scaling factors for analysis

    Returns:
    --------
    scalability_results : dict
        Scalability analysis across problem sizes
    """
    scalability_results = {}

    for factor in scaling_factors:
        problem_size = int(max_problem_size * factor)

        # Generate scaling problem
        problem = generate_scaling_problem(problem_size)

        # Benchmark at this scale
        with BlackwellMXFP8Context() as ctx:
            performance = benchmark_scaling_performance(problem)

        scalability_results[f"scale_{factor}x"] = {
            'problem_size': problem_size,
            'execution_time': performance['time'],
            'memory_usage': performance['memory'],
            'precision_maintained': performance['correlation'] >= 0.999744,
            'throughput_scaling': performance['throughput_scaling'],
            'efficiency_scaling': performance['efficiency_scaling']
        }

    return scalability_results
```

## Integration Quality Assurance

### Blackwell Compatibility Checklist
- [ ] **Hardware Detection**: Automatic Blackwell GPU detection and capability verification
- [ ] **MXFP8 Support**: Validation of MXFP8 format support and precision requirements
- [ ] **TMEM Utilization**: Optimization for 128×512 tensor memory layout
- [ ] **Precision Validation**: Correlation coefficient ≥ 0.999744 verification
- [ ] **Performance Benchmarking**: 3.4-3.7x speedup validation against FP32 baseline
- [ ] **Memory Optimization**: 75% memory reduction and efficient HBM3e utilization
- [ ] **Energy Efficiency**: 68-72% energy savings quantification
- [ ] **Error Handling**: Graceful fallback for non-Blackwell hardware

### Automated Validation Framework
```python
# ✅ CORRECT: Automated Blackwell integration validation
def validate_blackwell_integration(algorithm, test_suite):
    """
    Automated validation of Blackwell MXFP8 integration.

    Parameters:
    -----------
    algorithm : str
        Algorithm name to validate
    test_suite : dict
        Comprehensive test suite specification

    Returns:
    --------
    validation_report : dict
        Complete validation report
    """
    validation_report = {
        'hardware_compatibility': validate_hardware_requirements(),
        'mxfp8_precision': validate_precision_requirements(algorithm, test_suite),
        'performance_targets': validate_performance_targets(algorithm, test_suite),
        'memory_optimization': validate_memory_efficiency(algorithm, test_suite),
        'energy_efficiency': validate_energy_savings(algorithm, test_suite),
        'scalability_analysis': validate_scalability(algorithm, test_suite),
        'integration_quality': assess_integration_quality(algorithm, test_suite)
    }

    # Overall Blackwell compliance score
    compliance_score = sum(1 for result in validation_report.values() if result['passed']) / len(validation_report)

    validation_report['overall_compliance'] = {
        'score': compliance_score,
        'blackwell_ready': compliance_score >= 0.9,
        'recommendations': generate_compliance_recommendations(validation_report)
    }

    return validation_report
```

## Documentation Standards

### Blackwell Integration Documentation Template
```markdown
## Blackwell MXFP8 Integration

### Hardware Requirements
- **GPU**: Blackwell architecture (H100 series or newer)
- **Memory**: 192GB HBM3e minimum
- **Software**: PyTorch 2.1+ with Blackwell support
- **CUDA**: 12.1+ with Blackwell optimizations

### Precision Specifications
- **MXFP8 E4M3**: 4-bit exponent, 3-bit mantissa (±448 range)
- **MXFP8 E5M2**: 5-bit exponent, 2-bit mantissa (±57344 range)
- **Precision Target**: Correlation ≥ 0.999744 with FP32
- **Error Bound**: Relative error ≤ 0.000256

### Performance Characteristics
- **Speedup**: 3.4-3.7x improvement over Hopper BF16
- **Memory Reduction**: 75% decrease in memory usage
- **Energy Savings**: 68-72% reduction in computational energy
- **Throughput**: 2x increase in operations per second

### Implementation Guidelines
```python
# Blackwell MXFP8 optimization pattern
with BlackwellMXFP8Context() as ctx:
    # Quantize inputs to MXFP8
    A_mxfp8 = quantize_blackwell_mxfp8(A_fp32, 'E4M3')
    B_mxfp8 = quantize_blackwell_mxfp8(B_fp32, 'E4M3')

    # Perform optimized computation
    C_mxfp8 = blackwell_tensor_core_matmul(A_mxfp8, B_mxfp8)

    # Dequantize for precision-critical operations
    C_fp32 = dequantize_blackwell_mxfp8(C_mxfp8)

    # Validate precision preservation
    precision_ok = validate_mxfp8_precision(C_reference, C_fp32)
```

### Benchmarking Results
| Metric | MXFP8 Performance | FP32 Baseline | Improvement |
|--------|-------------------|---------------|-------------|
| Execution Time | 0.085s | 0.312s | 3.7x faster |
| Memory Usage | 18.2 MB | 72.8 MB | 75% reduction |
| Energy Consumption | 45.2 J | 162.8 J | 72% savings |
| Precision Correlation | 0.999744 | 1.0 | Within tolerance |

### Best Practices
1. **Quantization Strategy**: Use E4M3 for general computation, E5M2 for wide dynamic range
2. **Precision Monitoring**: Regularly validate correlation ≥ 0.999744
3. **Memory Layout**: Optimize for 128×512 TMEM access patterns
4. **Fallback Handling**: Implement FP32 fallback for non-Blackwell hardware
5. **Performance Profiling**: Monitor speedup and ensure ≥ 3.4x improvement

### Troubleshooting
- **Low Correlation**: Check quantization ranges and input normalization
- **Poor Speedup**: Verify Blackwell GPU availability and driver versions
- **Memory Issues**: Ensure proper TMEM utilization and HBM3e bandwidth
- **Compatibility Problems**: Confirm PyTorch Blackwell support and CUDA versions
```

This rule ensures Blackwell MXFP8 integration maintains the scientific computing toolkit's precision standards (0.9987 correlation) while delivering optimal performance improvements (3.5x speedup) and hardware efficiency (75% memory reduction, 68-72% energy savings).