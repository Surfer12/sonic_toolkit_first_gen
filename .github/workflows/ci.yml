name: Scientific Computing Toolkit CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-mock
        pip install numpy scipy torch matplotlib

    - name: Run comprehensive test suite
      run: |
        python tests/comprehensive_test_suite.py

    - name: Run integration tests
      run: |
        python complete_integration_test.py

    - name: Run performance benchmarks
      run: |
        python -c "
        import time
        import numpy as np
        from multi_algorithm_optimization import PrimeEnhancedOptimizer

        # Quick performance validation
        start_time = time.time()
        optimizer = PrimeEnhancedOptimizer()
        # Simulate optimization
        result = optimizer.optimize_with_prime_enhancement(
            lambda x: sum(x**2), [1.0, 1.0], bounds=([0, 0], [2, 2])
        )
        end_time = time.time()

        execution_time = end_time - start_time
        print(f'Performance benchmark: {execution_time:.3f}s')
        assert execution_time < 10.0, f'Performance regression: {execution_time}s'
        "

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          test_reports/
          integration_test_report.json
          data_output/logs/

  java-security-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'

    - name: Compile Java components
      run: |
        cd Corpus/qualia
        find . -name "*.java" -exec javac {} \;

    - name: Run Java tests
      run: |
        # Run Java unit tests if they exist
        find . -name "*Test.java" -exec java -cp . {} \;

  ios-swift-tests:
    runs-on: macos-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Swift
      run: |
        swift --version

    - name: Build Swift components
      run: |
        cd Farmer
        swift build

    - name: Run Swift tests
      run: |
        cd Farmer
        swift test

  cross-platform-integration:
    runs-on: ubuntu-latest
    needs: [test, java-security-tests]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test cross-framework communication
      run: |
        python -c "
        from cross_framework_communication import CrossFrameworkCommunicator
        import asyncio

        async def test_communication():
            communicator = CrossFrameworkCommunicator()
            # Test initialization
            assert hasattr(communicator, 'endpoints')
            assert hasattr(communicator, 'message_handlers')
            print('✅ Cross-framework communication initialized successfully')

        asyncio.run(test_communication())
        "

    - name: Validate integration configuration
      run: |
        python -c "
        import json
        from pathlib import Path

        # Check integration config
        config_path = Path('data_output/integration_config.json')
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
            assert 'data_flow_pipelines' in config
            assert 'processing_parameters' in config
            print('✅ Integration configuration validated')
        else:
            print('⚠️ Integration config not found, skipping validation')
        "

    - name: Test data pipeline
      run: |
        python -c "
        from pathlib import Path
        import json

        # Check data directories
        data_dirs = ['data', 'data_output']
        for dir_name in data_dirs:
            dir_path = Path(dir_name)
            if dir_path.exists():
                print(f'✅ {dir_name} directory exists')
            else:
                print(f'❌ {dir_name} directory missing')

        # Check key files
        key_files = [
            'complete_integration_test.py',
            'requirements.txt',
            'README.md'
        ]

        for file_name in key_files:
            file_path = Path(file_name)
            if file_path.exists():
                print(f'✅ {file_name} exists')
            else:
                print(f'❌ {file_name} missing')
        "

  performance-monitoring:
    runs-on: ubuntu-latest
    needs: [test]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil memory_profiler

    - name: Run performance benchmarks
      run: |
        python -c "
        import time
        import psutil
        import os
        from multi_algorithm_optimization import PrimeEnhancedOptimizer

        print('=== PERFORMANCE BENCHMARKS ===')

        # Memory usage
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024
        print(f'Memory before: {memory_before:.1f} MB')

        # Optimization benchmark
        start_time = time.time()
        optimizer = PrimeEnhancedOptimizer()
        result = optimizer.optimize_with_prime_enhancement(
            lambda x: sum(x**2 for x in x), [1.0, 1.0, 1.0],
            bounds=([0, 0, 0], [2, 2, 2])
        )
        end_time = time.time()

        memory_after = process.memory_info().rss / 1024 / 1024
        execution_time = end_time - start_time

        print(f'Optimization time: {execution_time:.3f}s')
        print(f'Memory after: {memory_after:.1f} MB')
        print(f'Memory delta: {memory_after - memory_before:.1f} MB')

        # Performance assertions
        assert execution_time < 5.0, f'Performance regression: {execution_time}s'
        assert memory_after < 200, f'Memory usage too high: {memory_after} MB'

        print('✅ Performance benchmarks passed')
        "

    - name: Monitor system resources
      run: |
        echo "=== SYSTEM RESOURCE MONITORING ==="
        df -h
        free -h
        echo "=== PROCESS INFORMATION ==="
        ps aux | head -10

  security-validation:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Security scan with Trivy
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run security checks
      run: |
        echo "=== SECURITY VALIDATION ==="

        # Check for sensitive files
        sensitive_files=$(find . -name "*.key" -o -name "*.pem" -o -name "*secret*" -o -name "*.env" | wc -l)
        if [ "$sensitive_files" -gt 0 ]; then
          echo "⚠️ Found $sensitive_files potentially sensitive files"
          find . -name "*.key" -o -name "*.pem" -o -name "*secret*" -o -name "*.env"
        else
          echo "✅ No sensitive files found"
        fi

        # Check file permissions
        echo "=== FILE PERMISSION CHECK ==="
        find . -type f -executable | head -10

        # Check for hardcoded secrets (basic check)
        secret_patterns="password|secret|key|token"
        secret_matches=$(grep -r -i "$secret_patterns" . --include="*.py" --include="*.java" --include="*.swift" | wc -l)
        if [ "$secret_matches" -gt 0 ]; then
          echo "⚠️ Found $secret_matches potential secret patterns (review manually)"
        else
          echo "✅ No obvious secret patterns found"
        fi

  documentation-validation:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Validate documentation
      run: |
        echo "=== DOCUMENTATION VALIDATION ==="

        # Check for required documentation files
        required_docs=("README.md" "docs/index.md" "requirements.txt")
        for doc in "${required_docs[@]}"; do
          if [ -f "$doc" ]; then
            echo "✅ $doc exists"
          else
            echo "❌ $doc missing"
          fi
        done

        # Check Python docstrings
        echo "=== PYTHON DOCSTRING CHECK ==="
        python_files=$(find . -name "*.py" -not -path "./.*" | wc -l)
        echo "Found $python_files Python files"

        # Basic documentation check
        undocumented_functions=$(python -c "
        import ast
        import os

        def check_file(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    tree = ast.parse(f.read())
                functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                documented = sum(1 for func in functions if ast.get_docstring(func))
                return len(functions), documented
            except:
                return 0, 0

        total_functions = 0
        total_documented = 0

        for root, dirs, files in os.walk('.'):
            if '.git' in dirs:
                dirs.remove('.git')
            for file in files:
                if file.endswith('.py') and not file.startswith('.'):
                    filepath = os.path.join(root, file)
                    functions, documented = check_file(filepath)
                    total_functions += functions
                    total_documented += documented

        if total_functions > 0:
            coverage = total_documented / total_functions * 100
            print(f'Functions: {total_functions}, Documented: {total_documented}, Coverage: {coverage:.1f}%')
        else:
            print('No functions found')
        ")

  release:
    runs-on: ubuntu-latest
    needs: [test, java-security-tests, ios-swift-tests, cross-platform-integration, performance-monitoring, security-validation, documentation-validation]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - uses: actions/checkout@v4

    - name: Create release archive
      run: |
        echo "=== CREATING RELEASE ARCHIVE ==="
        tar -czf scientific-computing-toolkit-${{ github.sha }}.tar.gz \
          --exclude='.git' \
          --exclude='.github' \
          --exclude='__pycache__' \
          --exclude='*.pyc' \
          --exclude='.DS_Store' \
          .

    - name: Upload release artifact
      uses: actions/upload-artifact@v3
      with:
        name: scientific-computing-toolkit-release
        path: scientific-computing-toolkit-${{ github.sha }}.tar.gz

    - name: Generate release notes
      run: |
        echo "=== RELEASE NOTES ==="
        echo "## Scientific Computing Toolkit Release"
        echo ""
        echo "### Changes in this release:"
        echo "- Enhanced cross-framework communication"
        echo "- Comprehensive test suite implementation"
        echo "- CI/CD pipeline for automated validation"
        echo "- Security measures for hybrid UQ predictions"
        echo "- Performance benchmarking across components"
        echo "- API documentation and user tutorials"
        echo ""
        echo "### Test Results:"
        echo "- Integration tests: ✅ PASSED"
        echo "- Performance benchmarks: ✅ PASSED"
        echo "- Security validation: ✅ PASSED"
        echo "- Documentation validation: ✅ PASSED"

  notify:
    runs-on: ubuntu-latest
    needs: [release]
    if: always()

    steps:
    - name: Post notification
      run: |
        echo "=== CI/CD PIPELINE COMPLETED ==="
        echo "Status: ${{ job.status }}"
        echo "Repository: ${{ github.repository }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"

        if [ "${{ job.status }}" = "success" ]; then
          echo "✅ All CI/CD checks passed successfully!"
        else
          echo "❌ CI/CD pipeline failed. Please check the logs above."
        fi